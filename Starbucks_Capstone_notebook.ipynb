{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This data set contains simulated data that mimics customer behavior on the Starbucks rewards mobile app. Once every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks. \n",
    "\n",
    "Not all users receive the same offer, and that is the challenge to solve with this data set.\n",
    "\n",
    "Your task is to combine transaction, demographic and offer data to determine which demographic groups respond best to which offer type. This data set is a simplified version of the real Starbucks app because the underlying simulator only has one product whereas Starbucks actually sells dozens of products.\n",
    "\n",
    "Every offer has a validity period before the offer expires. As an example, a BOGO offer might be valid for only 5 days. You'll see in the data set that informational offers have a validity period even though these ads are merely providing information about a product; for example, if an informational offer has 7 days of validity, you can assume the customer is feeling the influence of the offer for 7 days after receiving the advertisement.\n",
    "\n",
    "You'll be given transactional data showing user purchases made on the app including the timestamp of purchase and the amount of money spent on a purchase. This transactional data also has a record for each offer that a user receives as well as a record for when a user actually views the offer. There are also records for when a user completes an offer. \n",
    "\n",
    "Keep in mind as well that someone using the app might make a purchase through the app without having received an offer or seen an offer.\n",
    "\n",
    "### Example\n",
    "\n",
    "To give an example, a user could receive a discount offer buy 10 dollars get 2 off on Monday. The offer is valid for 10 days from receipt. If the customer accumulates at least 10 dollars in purchases during the validity period, the customer completes the offer.\n",
    "\n",
    "However, there are a few things to watch out for in this data set. Customers do not opt into the offers that they receive; in other words, a user can receive an offer, never actually view the offer, and still complete the offer. For example, a user might receive the \"buy 10 dollars get 2 dollars off offer\", but the user never opens the offer during the 10 day validity period. The customer spends 15 dollars during those ten days. There will be an offer completion record in the data set; however, the customer was not influenced by the offer because the customer never viewed the offer.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "This makes data cleaning especially important and tricky.\n",
    "\n",
    "You'll also want to take into account that some demographic groups will make purchases even if they don't receive an offer. From a business perspective, if a customer is going to make a 10 dollar purchase without an offer anyway, you wouldn't want to send a buy 10 dollars get 2 dollars off offer. You'll want to try to assess what a certain demographic group will buy when not receiving any offers.\n",
    "\n",
    "### Final Advice\n",
    "\n",
    "Because this is a capstone project, you are free to analyze the data any way you see fit. For example, you could build a machine learning model that predicts how much someone will spend based on demographics and offer type. Or you could build a model that predicts whether or not someone will respond to an offer. Or, you don't need to build a machine learning model at all. You could develop a set of heuristics that determine what offer you should send to each customer (i.e., 75 percent of women customers who were 35 years old responded to offer A vs 40 percent from the same demographic to offer B, so send offer A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets\n",
    "\n",
    "The data is contained in three files:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "Here is the schema and explanation of each variable in the files:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)\n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record\n",
    "\n",
    "**Note:** If you are using the workspace, you will need to go to the terminal and run the command `conda update pandas` before reading in the files. This is because the version of pandas in the workspace cannot read in the transcript.json file correctly, but the newest version of pandas can. You can access the termnal from the orange icon in the top left of this notebook.  \n",
    "\n",
    "You can see how to access the terminal and how the install works using the two images below.  First you need to access the terminal:\n",
    "\n",
    "<img src=\"pic1.png\"/>\n",
    "\n",
    "Then you will want to run the above command:\n",
    "\n",
    "<img src=\"pic2.png\"/>\n",
    "\n",
    "Finally, when you enter back into the notebook (use the jupyter icon again), you should be able to run the below cell without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load Starbucks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, json , re, os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, HTML, display\n",
    "from tqdm import tqdm\n",
    "from progressbar import progressbar\n",
    "\n",
    "# ML specific classes through sklearn\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# jupyter magic to display plots directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# use vector graphics format for nicer plots\n",
    "%config Inline.Backend.figure_format = 'svg'\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# read in the json files\n",
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Lets look at the data and check out inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Profile Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(profile.head(), profile.isna().sum().to_frame().rename(columns={0:'Null count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.query(\"age == 118\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We find there are null values in Gender (along with'O') and Income columns. As we can see above, age is 118 for all rows where null value is present in gender and income columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first rename id to customer_id for more clarity\n",
    "profile = profile.rename(columns={'id':'customer_id'})\n",
    "\n",
    "# No. of users \n",
    "print('profile size: {}'.format(profile['customer_id'].unique().shape[0]))\n",
    "\n",
    "# gender distribution\n",
    "display(profile['gender'].value_counts().to_frame())\n",
    "\n",
    "# income distribution by gender \n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=profile, x=\"income\", hue=\"gender\", kde=True);\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/profile_income_dist.svg')\n",
    "plt.show()\n",
    "\n",
    "# age distribution by gender\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=profile, x=\"age\", hue=\"gender\", kde=True);\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/profile_age_dist.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"became_member_on\" to standard python datetime format\n",
    "profile['became_member_on'] = pd.to_datetime(profile['became_member_on'], format = '%Y%m%d')\n",
    "profile['became_member_in'] = profile['became_member_on'].apply(lambda x: x.year)\n",
    "# Compute how long the user has been a member of the Starbucks app - let's say the anchor point is say 30-Sep-2020\n",
    "profile['member_since_in_months'] = (pd.to_datetime(\"30-Sep-2020\") - profile['became_member_on']).astype('timedelta64[M]').astype('int')\n",
    "\n",
    "profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"became member in\" distribution by gender - how many customers are new and how many are long term members?\n",
    "members_by_year = profile.groupby('became_member_in')['customer_id'].count().to_frame().rename(columns={'customer_id':'no_of_customers'})\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data = members_by_year, x=members_by_year.index, y='no_of_customers')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/profile_member_joined_dist.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"loyalty\" distribution by gender\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=profile, x=\"member_since_in_months\", kde=True);\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/profile_loyalty_dist.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Portfolio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(portfolio.head(), portfolio.isna().sum().to_frame().rename(columns={0:'Null count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first rename id to offer_id for more clarity\n",
    "portfolio = portfolio.rename(columns={'id':'offer_id'})\n",
    "\n",
    "# let's check the number of outstanding offers\n",
    "portfolio_stats = portfolio.groupby('offer_type')['offer_id'].count().to_frame()\n",
    "portfolio_stats.loc[\"total\"] = portfolio_stats.sum(axis=0)\n",
    "portfolio_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see multiple channels through which offers have been delivered to users - let's check them\n",
    "portfolio['channels'].explode().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also see that each offer type has a reward and difficulty columns - would be interesting to see if you get more reward for spending more? :)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(data=portfolio, x=\"difficulty\", y=\"reward\", hue=\"offer_type\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/portfolio_reward_vs_difficulty.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transcript Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(transcript.head(), transcript.isna().sum().to_frame().rename(columns={0:'Null count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first rename person to customer_id for more clarity\n",
    "transcript = transcript.rename(columns={'person':'customer_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of users \n",
    "print('transcript size: {}'.format(transcript['customer_id'].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It looks as though the number of people in transcript are the same as the number of people in the profile dataset, so that is good news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the different types of values - we're mainly interested in the dictionary keys\n",
    "dict_keys = [list(value.keys()) for value in transcript['value']]\n",
    "# need to flatten list of lists that may be present - occurs when a dictionary has multiple keys\n",
    "set([item for sublist in dict_keys for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event distribution\n",
    "event_dist = transcript.groupby('event')['customer_id'].count().to_frame()\n",
    "event_dist = event_dist.div(event_dist.sum(axis=0), axis=1).multiply(100)\n",
    "ax = event_dist.rename(columns={'customer_id':'pct'}).sort_values(by='pct', ascending=True).plot(kind='barh', figsize=(10,6), legend=None);\n",
    "plt.title('Events distribution', fontsize=14)\n",
    "plt.xlabel('percentage of total events')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/transcript_event_dist.svg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's expand values column and get keys as column headers and get rid of the dictionary\n",
    "transcript = pd.concat([transcript.drop(['value'], axis=1), transcript['value'].apply(pd.Series)], axis=1)\n",
    "# there are duplicate offer id and offer_id columns - need to clean them up\n",
    "transcript['offer_id'] = transcript['offer_id'].fillna(transcript['offer id'])\n",
    "transcript = transcript.drop(columns=['offer id'])\n",
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if no. of users in transcript is the same as in the profile dataset\n",
    "users_in_transcript = list(transcript['customer_id'].unique())\n",
    "users_in_profile = list(profile['customer_id'].unique())\n",
    "\n",
    "in_transcript_but_not_in_profile  = [x for x in users_in_transcript if x not in set(users_in_profile)]\n",
    "in_profile_but_not_in_transcript  = [x for x in users_in_profile if x not in set(users_in_transcript)]\n",
    "\n",
    "print('Number of users in transcript but not in profile dataset: '+str(len(in_transcript_but_not_in_profile)))\n",
    "print('Number of users in profile but not in transcript dataset: '+str(len(in_profile_but_not_in_transcript)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a temporary dataset to analyze events and event_type to see which offers do the users prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = transcript.copy()\n",
    "temp = temp.merge(portfolio[['offer_id','offer_type']], how='left', on='offer_id')\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_group = temp.groupby(['event','offer_type'])['event'].count().to_frame().rename(columns={'event':'count'})\n",
    "\n",
    "# get percentages event\n",
    "temp_group['percentage'] = temp_group.div(temp_group.sum(level=0), level=0)\n",
    "temp_group['percentage'].unstack(level=0).plot(kind='barh', subplots=True, figsize=(10,8));\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/transcript_portfolio_offer_preference.svg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning and preprocessing for machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets clean up all three dataframes into a usable format and join them in one single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Clean profile dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_preprocess_profile(profile):\n",
    "    \"\"\"\n",
    "    Takes the profile dataframe and cleans it by creating one-hot encodings as well as handling null values\n",
    "\n",
    "    \"\"\"\n",
    "    # We can safely remove null values since they form a very small part of the dataset\n",
    "    # Remove customers with null income data\n",
    "    profile = profile.dropna(subset=['gender','income'])\n",
    "    profile = profile[profile['gender'] != 'O']\n",
    "\n",
    "    # Specifying age range\n",
    "    min_age = np.int(np.floor(np.min(profile['age'])/10)*10)\n",
    "    max_age = np.int(np.ceil(np.max(profile['age'])/10)*10)\n",
    "    age_bins = range(min_age, max_age + 10, 10)\n",
    "    \n",
    "    profile['age_range'] = pd.cut(x=profile['age'], bins=age_bins, right=True).astype('str')\n",
    "    # One-hot encode age_range column\n",
    "    age_dummies = pd.get_dummies(profile['age_range'])\n",
    "    \n",
    "    # Specifying income range and one hot encoding\n",
    "    min_income = np.floor(np.min(profile['income']))\n",
    "    max_income = np.ceil(np.max(profile['income']))\n",
    "    income_bins = np.linspace(min_income, max_income, 10, endpoint=True)\n",
    "    \n",
    "    profile['income_range'] = pd.cut(x=profile['income'], bins=income_bins, right=True, include_lowest=True).astype('str')\n",
    "    # One-hot encode income_range column\n",
    "    income_dummies = pd.get_dummies(profile['income_range'])\n",
    "    \n",
    "    # Finally, transform gender from a character to a number using LabelBinarizer\n",
    "    convert_to_binary = LabelBinarizer()\n",
    "    profile['gender'] = convert_to_binary.fit_transform(profile['gender'])\n",
    "    \n",
    "    gender_map = {}\n",
    "    for gender_int in convert_to_binary.classes_:\n",
    "        gender_map[gender_int] = convert_to_binary.transform([gender_int])[0,0]\n",
    "    \n",
    "    # the year that a customer became a member in is not uniformly distributed (6th cell) - potential customer differentiator\n",
    "    # let's add dummies for that as well\n",
    "    year_dummies = pd.get_dummies(profile['became_member_in'])\n",
    "    \n",
    "    # concat the dummies to the profile dataset and drop the original columns\n",
    "    profile = (pd.concat([profile, age_dummies, income_dummies, year_dummies], axis=1, sort=False)\n",
    "                .drop(columns=['age','age_range','income','income_range',\n",
    "                               'became_member_on','became_member_in','member_since_in_months']))\n",
    "    \n",
    "    return profile, gender_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_profile, gender_map = clean_preprocess_profile(profile)\n",
    "print(gender_map)\n",
    "cleaned_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_profile.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clean portfolio dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_preprocess_portfolio(portfolio):\n",
    "    \"\"\"\n",
    "    Takes the portfolio dataframe and cleans it by creating one-hot encodings\n",
    "    \n",
    "    \"\"\"\n",
    "    # No null handling required since there are no NULL values\n",
    "    # One-hot encode channels column - using sckit-learn module\n",
    "    multi_label_binary = MultiLabelBinarizer()\n",
    "    multi_label_binary.fit(portfolio['channels'])\n",
    "    \n",
    "    channels_dummies = pd.DataFrame(multi_label_binary.transform(portfolio['channels']), columns=multi_label_binary.classes_)\n",
    "    \n",
    "    # One-hot encode offer_type column\n",
    "    offer_type_dummies = pd.get_dummies(portfolio['offer_type'])\n",
    "    \n",
    "    # that's it - concat now\n",
    "    portfolio = pd.concat([portfolio, channels_dummies, offer_type_dummies], axis=1, sort=False) \\\n",
    "                  .drop(columns=['offer_type', 'channels'])\n",
    "    \n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_portfolio = clean_preprocess_portfolio(portfolio)\n",
    "cleaned_portfolio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clean transcript dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_preprocess_transcript(transcript):\n",
    "    \"\"\" \n",
    "    Transforms the transcript dataframe and clean it by creating one-hot encodings \n",
    "    Also split the dataframe into seperate offers and transaction datasets\n",
    "    \n",
    "    \"\"\"\n",
    "    # no need to clean for customers not in profile since both have the same users - cell 18\n",
    "    # Convert time column from hours to days\n",
    "    transcript['time'] = transcript['time'] / 24.0\n",
    "    \n",
    "    # let's first get dummies for events\n",
    "    event_dummies = pd.get_dummies(transcript['event'])\n",
    "    event_dummies.columns = [col.replace(' ','_') for col in event_dummies.columns]\n",
    "\n",
    "    # now concat to get final dataframe\n",
    "    cleaned_transcript = pd.concat([transcript, event_dummies], axis=1, sort=False).drop(columns=['event'])\n",
    "    \n",
    "    offer_cols = ['customer_id','offer_id','time','offer_completed','offer_received','offer_viewed']\n",
    "    transac_cols = ['customer_id','time','amount','reward']\n",
    "    cleaned_offer_dataset = cleaned_transcript.query(\"offer_completed==1 or offer_received==1 or offer_viewed==1\")[offer_cols]\n",
    "    cleaned_transaction_dataset = cleaned_transcript.query(\"transaction==1\")[transac_cols]\n",
    "    \n",
    "    return cleaned_transcript, cleaned_offer_dataset, cleaned_transaction_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_transcript, cleaned_offer_dataset, cleaned_transaction_dataset = clean_preprocess_transcript(transcript)\n",
    "cleaned_transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_offer_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_transaction_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obersevations\n",
    "#### What is an effective offer?\n",
    "1. For BOGO and discount offers, an effective offer would be defined if the events are defined in this chronological order:\n",
    "   **offer received > offer viewed > transaction > offer completed**\n",
    "2. For Informational offer, the event chronology would be:\n",
    "   **offer received > offer viewed > transaction**\n",
    "   \n",
    "#### What is not counted as an effective offer?\n",
    "1. User recieved and viewed the offer but did not transact: **offer received > offer viewed** (no offer completed/transaction events)\n",
    "2. User received offer but did not do anything: **offer received** (did not even view it)\n",
    "3. Users who transacted w/o receiving an offer, after the offer is completed, before the offer is received or before the the offer is viewed. The different timelines can be summarized as:\n",
    "    - **transaction**\n",
    "    - **offer received > \"do nothing\"** (did not even view the offer)\n",
    "    - **offer received > transaction > offer completed > offer viewed**\n",
    "    - **transaction > offer received > offer completed > offer viewed**\n",
    "    - **offer received > transaction > offer viewed > offer completed**\n",
    "    - **offer received > transaction** (only applicable to informational offer)\n",
    "    - **offer received > transaction > offer viewed** (only applicable to informational offer)\n",
    "    \n",
    "(3) can be summarized as - if an offer completed or transaction event occurs before an offer viewed event occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's sort our dataframe by customer_id and time and iterate through each customer id to label the dataset (computationally intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_effective_offer_label(profile, portfolio, offers, transactions):\n",
    "    # define the unique customer_ids to loop over\n",
    "    customer_ids = offers['customer_id'].unique().tolist()\n",
    "    \n",
    "    all_offers_labeled = []\n",
    "    for _user in tqdm(range(len(customer_ids))):\n",
    "        events = ['offer_completed','offer_received','offer_viewed']\n",
    "        \n",
    "        # get customer_id from the list\n",
    "        user_id = customer_ids[_user]\n",
    "        \n",
    "        # get profile data for the user\n",
    "        user_profile = profile.query(\"customer_id == @user_id\").drop(columns='customer_id')\n",
    "        # get offer data for the user\n",
    "        user_offers_data = offers.query(\"customer_id == @user_id\").drop(columns='customer_id')\n",
    "        # get transaction data for the user\n",
    "        user_transactions_data = transactions.query(\"customer_id == @user_id\").drop(columns='customer_id')\n",
    "        \n",
    "        offer_received = offers.query(\"offer_received == 1\").drop(columns=events)\n",
    "        offer_viewed = offers.query(\"offer_viewed == 1\").drop(columns=events)\n",
    "        offer_completed = offers.query(\"offer_completed == 1\").drop(columns=events)\n",
    "        \n",
    "        # loop over each offer for the particular user\n",
    "        all_offers_user = []\n",
    "        for offer in range(len(offer_received)):\n",
    "            # fetch offer id for the offer \n",
    "            offer_id = offer_received.iloc[offer]['offer_id']\n",
    "            \n",
    "            # extract offer data from portfolio\n",
    "            offer_row = portfolio.query(\"offer_id == @offer_id\").drop(columns=['offer_id'])\n",
    "            \n",
    "            # extract offer duration from offer row\n",
    "            offer_duration = offer_row['duration'].values[0]\n",
    "\n",
    "            # compute offer start and offer end times (in days)\n",
    "            start_offer = offer_received.iloc[offer]['time']\n",
    "            end_offer = start_offer + offer_duration\n",
    "            \n",
    "            # check if offer was viewed/completed and was transcated for the duration of the offer\n",
    "            offer_transactions = user_transactions_data.query(\"time >= @start_offer and time <= @end_offer\")\n",
    "            \n",
    "            offer_viewed_mask = (offer_viewed['time'] >= start_offer) & (offer_viewed['time'] <= end_offer)\n",
    "            offer_completed_mask = (offer_completed['time'] >= start_offer) & (offer_completed['time'] <= end_offer)\n",
    "            offer_successful_mask = (offer_viewed_mask.sum() > 0) & (offer_completed_mask.sum() > 0)\n",
    "            \n",
    "            # make a dictionary that describes the current customer offer\n",
    "            offer_summary = {'effective_offer': int(offer_successful_mask), 'offer_id': offer_id, 'customer_id': user_id, \n",
    "                             'time': start_offer, 'total_amount_transacted_for_offer': offer_transactions['amount'].sum()}\n",
    "            \n",
    "            offer_summary_df = pd.DataFrame([offer_summary])\n",
    "            # prepare to concat the features now from offer_row and user_offers_data\n",
    "            offer_labeled = (pd.concat([offer_summary_df, offer_row, user_profile], axis=1, sort=False))\n",
    "            \n",
    "            all_offers_user.append(offer_labeled)\n",
    "        \n",
    "        # aggregate all offers for the user\n",
    "        user_offers_labeled = pd.concat(all_offers_user)        \n",
    "        \n",
    "        all_offers_labeled.append(user_offers_labeled)\n",
    "    \n",
    "    # aggregate for all users/customer_ids\n",
    "    all_offers_labeled_df = pd.concat(all_offers_labeled)\n",
    "    \n",
    "    return all_offers_labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "effective_offer_labeled_dataset = (construct_effective_offer_label(cleaned_profile, \n",
    "                                                                   cleaned_portfolio, \n",
    "                                                                   cleaned_offer_dataset, \n",
    "                                                                   cleaned_transaction_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_offer_labeled_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_offer_labeled_dataset.to_pickle('effective_offer_labeled_dataset.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
